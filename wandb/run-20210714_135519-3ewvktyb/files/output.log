
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
Generator is created!
Initialize generator with xavier type
Discriminator is created!
Initialize discriminator with xavier type
Traceback (most recent call last):
  File "train.py", line 78, in <module>
    trainer.WGAN_trainer(opt)
  File "/workspace/jaejun/inpainting/code/trainer.py", line 45, in WGAN_trainer
    generator = generator.cuda()
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 637, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 552, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 637, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.