{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary\n",
    "\n",
    "import easydict\n",
    "\n",
    "from network_module import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = easydict.EasyDict({\n",
    "    \"data_dir\": '../dataset',\n",
    "    \"input_length\": 220500,\n",
    "    \"image_height\": 1025,\n",
    "    \"image_width\": 431,\n",
    "    \"bbox_shape\": 120,\n",
    "    \"mask_type\": 'time_masking',\n",
    "    \"in_channels\" : 2,\n",
    "    \"out_channels\" : 1,\n",
    "    \"latent_channels\" : 64,\n",
    "    \"pad_type\": 'zero',\n",
    "    \"activation\": 'lrelu',\n",
    "    \"norm\":'in',\n",
    "    \"init_type\":'xavier',\n",
    "    \"init_gain\":0.02\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "    Parameters:\n",
    "        net (network)       -- network to be initialized\n",
    "        init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "    \"\"\"\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            init.normal_(m.weight, 0, 0.01)\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "    # Apply the initialization function <init_func>\n",
    "    net.apply(init_func)\n",
    "\n",
    "#-----------------------------------------------\n",
    "#                   Generator\n",
    "#-----------------------------------------------\n",
    "# Input: masked image + mask\n",
    "# Output: filled image\n",
    "class GatedGenerator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(GatedGenerator, self).__init__()\n",
    "        self.coarse = nn.Sequential(\n",
    "            # encoder\n",
    "            GatedConv2d(opt.in_channels, opt.latent_channels, 7, 1, 3, pad_type = opt.pad_type, activation = opt.activation, norm = 'none'),\n",
    "            GatedConv2d(opt.latent_channels, opt.latent_channels * 2, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 2, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            # Bottleneck\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 2, dilation = 2, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 4, dilation = 4, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 8, dilation = 8, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 16, dilation = 16, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            # decoder\n",
    "            TransposeGatedConv2d(opt.latent_channels * 4, opt.latent_channels * 2, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 2, opt.latent_channels * 2, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            TransposeGatedConv2d(opt.latent_channels * 2, opt.latent_channels, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels, opt.out_channels, 7, 1, 3, pad_type = opt.pad_type, activation = 'none', norm = 'none')\n",
    "        )\n",
    "        self.refinement = nn.Sequential(\n",
    "            # encoder\n",
    "            GatedConv2d(opt.in_channels, opt.latent_channels, 7, 1, 3, pad_type = opt.pad_type, activation = opt.activation, norm = 'none'),\n",
    "            GatedConv2d(opt.latent_channels, opt.latent_channels * 2, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 2, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            # Bottleneck\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 2, dilation = 2, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 4, dilation = 4, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 8, dilation = 8, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 16, dilation = 16, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 4, opt.latent_channels * 4, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            # decoder\n",
    "            TransposeGatedConv2d(opt.latent_channels * 4, opt.latent_channels * 2, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels * 2, opt.latent_channels * 2, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            TransposeGatedConv2d(opt.latent_channels * 2, opt.latent_channels, 3, 1, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm),\n",
    "            GatedConv2d(opt.latent_channels, opt.out_channels, 7, 1, 3, pad_type = opt.pad_type, activation = 'none', norm = 'none')\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, mask):\n",
    "        # img: entire img\n",
    "        # mask: 1 for mask region; 0 for unmask region\n",
    "        # 1 - mask: unmask\n",
    "        # img * (1 - mask): ground truth unmask region\n",
    "        # Coarse\n",
    "        print(img.shape, mask.shape)\n",
    "        first_masked_img = img * (1 - mask) + mask\n",
    "        first_in = torch.cat((first_masked_img, mask), 1)       # in: [B, 4, H, W]\n",
    "        first_out = self.coarse(first_in)                       # out: [B, 3, H, W]\n",
    "        # Refinement\n",
    "        second_masked_img = img * (1 - mask) + first_out * mask\n",
    "        second_in = torch.cat((second_masked_img, mask), 1)     # in: [B, 4, H, W]\n",
    "        second_out = self.refinement(second_in)                 # out: [B, 3, H, W]\n",
    "        return first_out, second_out\n",
    "    \n",
    "\n",
    "#-----------------------------------------------\n",
    "#                  Discriminator\n",
    "#-----------------------------------------------\n",
    "# Input: generated image / ground truth and mask\n",
    "# Output: patch based region, we set 30 * 30\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        # Down sampling\n",
    "        self.block1 = Conv2dLayer(opt.in_channels, opt.latent_channels, 7, 1, 3, pad_type = opt.pad_type, activation = opt.activation, norm = 'none', sn = True)\n",
    "        self.block2 = Conv2dLayer(opt.latent_channels, opt.latent_channels * 2, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm, sn = True)\n",
    "        self.block3 = Conv2dLayer(opt.latent_channels * 2, opt.latent_channels * 4, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm, sn = True)\n",
    "        self.block4 = Conv2dLayer(opt.latent_channels * 4, opt.latent_channels * 4, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm, sn = True)\n",
    "        self.block5 = Conv2dLayer(opt.latent_channels * 4, opt.latent_channels * 4, 4, 2, 1, pad_type = opt.pad_type, activation = opt.activation, norm = opt.norm, sn = True)\n",
    "        self.block6 = Conv2dLayer(opt.latent_channels * 4, 1, 4, 2, 1, pad_type = opt.pad_type, activation = 'none', norm = 'none', sn = True)\n",
    "        \n",
    "    def forward(self, img, mask):\n",
    "        # the input x should contain 4 channels because it is a combination of recon image and mask\n",
    "        x = torch.cat((img, mask), 1)\n",
    "        x = self.block1(x)                                      # out: [B, 64, 256, 256]\n",
    "        x = self.block2(x)                                      # out: [B, 128, 128, 128]\n",
    "        x = self.block3(x)                                      # out: [B, 256, 64, 64]\n",
    "        x = self.block4(x)                                      # out: [B, 256, 32, 32]\n",
    "        x = self.block5(x)                                      # out: [B, 256, 16, 16]\n",
    "        x = self.block6(x)                                      # out: [B, 256, 8, 8]\n",
    "        return x\n",
    "\n",
    "# ----------------------------------------\n",
    "#            Perceptual Network\n",
    "# ----------------------------------------\n",
    "# VGG-16 conv4_3 features\n",
    "class PerceptualNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GatedGenerator(opt)\n",
    "discriminator = PatchDiscriminator(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 428]) torch.Size([2, 1, 1024, 428])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1         [-1, 2, 1030, 434]               0\n",
      "            Conv2d-2        [-1, 64, 1024, 428]           6,336\n",
      "            Conv2d-3        [-1, 64, 1024, 428]           6,336\n",
      "           Sigmoid-4        [-1, 64, 1024, 428]               0\n",
      "         LeakyReLU-5        [-1, 64, 1024, 428]               0\n",
      "       GatedConv2d-6        [-1, 64, 1024, 428]               0\n",
      "         ZeroPad2d-7        [-1, 64, 1026, 430]               0\n",
      "            Conv2d-8        [-1, 128, 512, 214]         131,200\n",
      "            Conv2d-9        [-1, 128, 512, 214]         131,200\n",
      "          Sigmoid-10        [-1, 128, 512, 214]               0\n",
      "   InstanceNorm2d-11        [-1, 128, 512, 214]               0\n",
      "        LeakyReLU-12        [-1, 128, 512, 214]               0\n",
      "      GatedConv2d-13        [-1, 128, 512, 214]               0\n",
      "        ZeroPad2d-14        [-1, 128, 514, 216]               0\n",
      "           Conv2d-15        [-1, 256, 512, 214]         295,168\n",
      "           Conv2d-16        [-1, 256, 512, 214]         295,168\n",
      "          Sigmoid-17        [-1, 256, 512, 214]               0\n",
      "   InstanceNorm2d-18        [-1, 256, 512, 214]               0\n",
      "        LeakyReLU-19        [-1, 256, 512, 214]               0\n",
      "      GatedConv2d-20        [-1, 256, 512, 214]               0\n",
      "        ZeroPad2d-21        [-1, 256, 514, 216]               0\n",
      "           Conv2d-22        [-1, 256, 256, 107]       1,048,832\n",
      "           Conv2d-23        [-1, 256, 256, 107]       1,048,832\n",
      "          Sigmoid-24        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-25        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-26        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-27        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-28        [-1, 256, 258, 109]               0\n",
      "           Conv2d-29        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-30        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-31        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-32        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-33        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-34        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-35        [-1, 256, 258, 109]               0\n",
      "           Conv2d-36        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-37        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-38        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-39        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-40        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-41        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-42        [-1, 256, 260, 111]               0\n",
      "           Conv2d-43        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-44        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-45        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-46        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-47        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-48        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-49        [-1, 256, 264, 115]               0\n",
      "           Conv2d-50        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-51        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-52        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-53        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-54        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-55        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-56        [-1, 256, 272, 123]               0\n",
      "           Conv2d-57        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-58        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-59        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-60        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-61        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-62        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-63        [-1, 256, 288, 139]               0\n",
      "           Conv2d-64        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-65        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-66        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-67        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-68        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-69        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-70        [-1, 256, 258, 109]               0\n",
      "           Conv2d-71        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-72        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-73        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-74        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-75        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-76        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-77        [-1, 256, 258, 109]               0\n",
      "           Conv2d-78        [-1, 256, 256, 107]         590,080\n",
      "           Conv2d-79        [-1, 256, 256, 107]         590,080\n",
      "          Sigmoid-80        [-1, 256, 256, 107]               0\n",
      "   InstanceNorm2d-81        [-1, 256, 256, 107]               0\n",
      "        LeakyReLU-82        [-1, 256, 256, 107]               0\n",
      "      GatedConv2d-83        [-1, 256, 256, 107]               0\n",
      "        ZeroPad2d-84        [-1, 256, 514, 216]               0\n",
      "     SpectralNorm-85        [-1, 128, 512, 214]               0\n",
      "     SpectralNorm-86        [-1, 128, 512, 214]               0\n",
      "          Sigmoid-87        [-1, 128, 512, 214]               0\n",
      "   InstanceNorm2d-88        [-1, 128, 512, 214]               0\n",
      "        LeakyReLU-89        [-1, 128, 512, 214]               0\n",
      "      GatedConv2d-90        [-1, 128, 512, 214]               0\n",
      "TransposeGatedConv2d-91        [-1, 128, 512, 214]               0\n",
      "        ZeroPad2d-92        [-1, 128, 514, 216]               0\n",
      "           Conv2d-93        [-1, 128, 512, 214]         147,584\n",
      "           Conv2d-94        [-1, 128, 512, 214]         147,584\n",
      "          Sigmoid-95        [-1, 128, 512, 214]               0\n",
      "   InstanceNorm2d-96        [-1, 128, 512, 214]               0\n",
      "        LeakyReLU-97        [-1, 128, 512, 214]               0\n",
      "      GatedConv2d-98        [-1, 128, 512, 214]               0\n",
      "        ZeroPad2d-99       [-1, 128, 1026, 430]               0\n",
      "    SpectralNorm-100        [-1, 64, 1024, 428]               0\n",
      "    SpectralNorm-101        [-1, 64, 1024, 428]               0\n",
      "         Sigmoid-102        [-1, 64, 1024, 428]               0\n",
      "  InstanceNorm2d-103        [-1, 64, 1024, 428]               0\n",
      "       LeakyReLU-104        [-1, 64, 1024, 428]               0\n",
      "     GatedConv2d-105        [-1, 64, 1024, 428]               0\n",
      "TransposeGatedConv2d-106        [-1, 64, 1024, 428]               0\n",
      "       ZeroPad2d-107        [-1, 64, 1030, 434]               0\n",
      "          Conv2d-108         [-1, 1, 1024, 428]           3,137\n",
      "          Conv2d-109         [-1, 1, 1024, 428]           3,137\n",
      "         Sigmoid-110         [-1, 1, 1024, 428]               0\n",
      "     GatedConv2d-111         [-1, 1, 1024, 428]               0\n",
      "       ZeroPad2d-112         [-1, 2, 1030, 434]               0\n",
      "          Conv2d-113        [-1, 64, 1024, 428]           6,336\n",
      "          Conv2d-114        [-1, 64, 1024, 428]           6,336\n",
      "         Sigmoid-115        [-1, 64, 1024, 428]               0\n",
      "       LeakyReLU-116        [-1, 64, 1024, 428]               0\n",
      "     GatedConv2d-117        [-1, 64, 1024, 428]               0\n",
      "       ZeroPad2d-118        [-1, 64, 1026, 430]               0\n",
      "          Conv2d-119        [-1, 128, 512, 214]         131,200\n",
      "          Conv2d-120        [-1, 128, 512, 214]         131,200\n",
      "         Sigmoid-121        [-1, 128, 512, 214]               0\n",
      "  InstanceNorm2d-122        [-1, 128, 512, 214]               0\n",
      "       LeakyReLU-123        [-1, 128, 512, 214]               0\n",
      "     GatedConv2d-124        [-1, 128, 512, 214]               0\n",
      "       ZeroPad2d-125        [-1, 128, 514, 216]               0\n",
      "          Conv2d-126        [-1, 256, 512, 214]         295,168\n",
      "          Conv2d-127        [-1, 256, 512, 214]         295,168\n",
      "         Sigmoid-128        [-1, 256, 512, 214]               0\n",
      "  InstanceNorm2d-129        [-1, 256, 512, 214]               0\n",
      "       LeakyReLU-130        [-1, 256, 512, 214]               0\n",
      "     GatedConv2d-131        [-1, 256, 512, 214]               0\n",
      "       ZeroPad2d-132        [-1, 256, 514, 216]               0\n",
      "          Conv2d-133        [-1, 256, 256, 107]       1,048,832\n",
      "          Conv2d-134        [-1, 256, 256, 107]       1,048,832\n",
      "         Sigmoid-135        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-136        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-137        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-138        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-139        [-1, 256, 258, 109]               0\n",
      "          Conv2d-140        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-141        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-142        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-143        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-144        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-145        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-146        [-1, 256, 258, 109]               0\n",
      "          Conv2d-147        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-148        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-149        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-150        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-151        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-152        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-153        [-1, 256, 260, 111]               0\n",
      "          Conv2d-154        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-155        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-156        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-157        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-158        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-159        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-160        [-1, 256, 264, 115]               0\n",
      "          Conv2d-161        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-162        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-163        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-164        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-165        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-166        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-167        [-1, 256, 272, 123]               0\n",
      "          Conv2d-168        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-169        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-170        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-171        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-172        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-173        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-174        [-1, 256, 288, 139]               0\n",
      "          Conv2d-175        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-176        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-177        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-178        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-179        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-180        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-181        [-1, 256, 258, 109]               0\n",
      "          Conv2d-182        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-183        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-184        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-185        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-186        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-187        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-188        [-1, 256, 258, 109]               0\n",
      "          Conv2d-189        [-1, 256, 256, 107]         590,080\n",
      "          Conv2d-190        [-1, 256, 256, 107]         590,080\n",
      "         Sigmoid-191        [-1, 256, 256, 107]               0\n",
      "  InstanceNorm2d-192        [-1, 256, 256, 107]               0\n",
      "       LeakyReLU-193        [-1, 256, 256, 107]               0\n",
      "     GatedConv2d-194        [-1, 256, 256, 107]               0\n",
      "       ZeroPad2d-195        [-1, 256, 514, 216]               0\n",
      "    SpectralNorm-196        [-1, 128, 512, 214]               0\n",
      "    SpectralNorm-197        [-1, 128, 512, 214]               0\n",
      "         Sigmoid-198        [-1, 128, 512, 214]               0\n",
      "  InstanceNorm2d-199        [-1, 128, 512, 214]               0\n",
      "       LeakyReLU-200        [-1, 128, 512, 214]               0\n",
      "     GatedConv2d-201        [-1, 128, 512, 214]               0\n",
      "TransposeGatedConv2d-202        [-1, 128, 512, 214]               0\n",
      "       ZeroPad2d-203        [-1, 128, 514, 216]               0\n",
      "          Conv2d-204        [-1, 128, 512, 214]         147,584\n",
      "          Conv2d-205        [-1, 128, 512, 214]         147,584\n",
      "         Sigmoid-206        [-1, 128, 512, 214]               0\n",
      "  InstanceNorm2d-207        [-1, 128, 512, 214]               0\n",
      "       LeakyReLU-208        [-1, 128, 512, 214]               0\n",
      "     GatedConv2d-209        [-1, 128, 512, 214]               0\n",
      "       ZeroPad2d-210       [-1, 128, 1026, 430]               0\n",
      "    SpectralNorm-211        [-1, 64, 1024, 428]               0\n",
      "    SpectralNorm-212        [-1, 64, 1024, 428]               0\n",
      "         Sigmoid-213        [-1, 64, 1024, 428]               0\n",
      "  InstanceNorm2d-214        [-1, 64, 1024, 428]               0\n",
      "       LeakyReLU-215        [-1, 64, 1024, 428]               0\n",
      "     GatedConv2d-216        [-1, 64, 1024, 428]               0\n",
      "TransposeGatedConv2d-217        [-1, 64, 1024, 428]               0\n",
      "       ZeroPad2d-218        [-1, 64, 1030, 434]               0\n",
      "          Conv2d-219         [-1, 1, 1024, 428]           3,137\n",
      "          Conv2d-220         [-1, 1, 1024, 428]           3,137\n",
      "         Sigmoid-221         [-1, 1, 1024, 428]               0\n",
      "     GatedConv2d-222         [-1, 1, 1024, 428]               0\n",
      "================================================================\n",
      "Total params: 25,411,588\n",
      "Trainable params: 25,411,588\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 732736.00\n",
      "Forward/backward pass size (MB): 21576.31\n",
      "Params size (MB): 96.94\n",
      "Estimated Total Size (MB): 754409.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(generator, [(1, 1024, 428), (1, 1024, 428)], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "one = torch.ones([1])*0.581\n",
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.append(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5810)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "img = torch.ones(1, 2, 513, 431)\n",
    "mask = torch.zeros(1, 1, 513, 431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 513, 431])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_masked_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512, 428])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = img[0,0,:,:]\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "axs.set_title('Spectrogram (db)')\n",
    "axs.set_ylabel('freq_bin')\n",
    "axs.set_xlabel('frame')\n",
    "im = axs.imshow(librosa.amplitude_to_db(t), origin='lower', aspect='auto')\n",
    "if None:\n",
    "    axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "plt.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
